{"dataset": "MHD_256", "url": "https://iopscience.iop.org/article/10.3847/1538-4357/abc484/pdf", "links": "https://iopscience.iop.org/article/10.3847/1538-4357/abc484/pdf", "abstract": "An essential component of the solar wind, galaxy formation, and of interstellar medium (ISM) dynamics is magnetohydrodynamic (MHD) turbulence. This dataset consists of isothermal MHD simulations without self-gravity (such as found in the diffuse ISM) initially generated with resolution $256^3$ and then downsampled to $64^3$ after anti-aliasing with an ideal low-pass filter.\nDimension of discretized data: 100 timesteps of $256\\times 256\\times256$ cubes.\nFields available in the data: Density (scalar field), velocity (vector field), magnetic field (vector field).\nNumber of trajectories: 10 Initial conditions x 10 combination of parameters = 100 trajectories.\nEstimated size of the ensemble of all simulations: 4.58TB.\nGrid type: uniform grid, cartesian coordinates.\nInitial conditions: uniform IC.\nBoundary conditions: periodic boundary conditions.\nData are stored separated by ($\\Delta t$): 0.01 (arbitrary units).\nTotal time range ($t\\_{min}$ to $t\\_{max}$): $t\\_{min} = 0$, $t\\_{max} = 1$.\nSpatial domain size ($L_x$, $L_y$, $L_z$): dimensionless so 256 pixels.\nSet of coefficients or non-dimensional parameters evaluated: all combinations of $\\mathcal{M}_s=${0.5, 0.7, 1.5, 2.0 7.0} and $\\mathcal{M}_A =${0.7, 2.0}.\nApproximate time to generate the data: 48 hours per simulation.\nHardware used to generate the data**: 64 cores.\nWhat phenomena of physical interest are catpured in the data: MHD fluid flows in the compressible limit (sub and super sonic, sub and super Alfvenic).\nHow to evaluate a new simulator operating in this space: Check metrics such as Power spectrum, two points correlation function.", "id": "Magnetohydrodynamics (MHD) compressible turbulence", "primary_category": "physics.flu-dyn", "authors": "['B. Burkhart',  'S. M. Appel',  'S. Bialy',  'J. Cho',  'A. J. Christensen',  'D. Collins',  'C. Federrath',  'D. B. Fielding',  'D. Finkbeiner',  'A. S. Hill',  'J. C. Ibáñez-Mejía',  'M. R. Krumholz',  'A. Lazarian',  'M. Li',  'P. Mocz',  'M.-M. Mac Low',  'J. Naiman',  'S. K. N. Portillo',  'B. Shane',  'Z. Slepian',  'Y. Yuan']"}
{"dataset": "MHD_64", "url": "https://iopscience.iop.org/article/10.3847/1538-4357/abc484/pdf", "links": "https://iopscience.iop.org/article/10.3847/1538-4357/abc484/pdf", "abstract": "An essential component of the solar wind, galaxy formation, and of interstellar medium (ISM) dynamics is magnetohydrodynamic (MHD) turbulence. This dataset consists of isothermal MHD simulations without self-gravity (such as found in the diffuse ISM) initially generated with resolution $256^3$ and then downsampled to $64^3$ after anti-aliasing with an ideal low-pass filter. This dataset is the downsampled version.\nDimension of discretized data: 100 timesteps of $64\\times 64\\times 64$ cubes.\nFields available in the data: Density (scalar field), velocity (vector field), magnetic field (vector field).\nNumber of trajectories: 10 Initial conditions x 10 combination of parameters = 100 trajectories.\nEstimated size of the ensemble of all simulations: 71.6 GB.\nGrid type: uniform grid, cartesian coordinates.\nInitial conditions: uniform IC.\nBoundary conditions: periodic boundary conditions.\nData are stored separated by ($\\Delta t$): 0.01 (arbitrary units).\nTotal time range ($t\\_{min}$ to $t\\_{max}$): $t\\_{min} = 0$, $t\\_{max} = 1$.\nSpatial domain size ($L_x$, $L_y$, $L_z$): dimensionless so 64 pixels.\nSet of coefficients or non-dimensional parameters evaluated: all combinations of $\\mathcal{M}_s=${0.5, 0.7, 1.5, 2.0 7.0} and $\\mathcal{M}_A =${0.7, 2.0}.\nApproximate time to generate the data: Downsampled from MHD_256 after applying ideal low-pass filter.\nHardware used to generate the data**: Downsampled from MHD_256 after applying ideal low-pass filter.\nWhat phenomena of physical interest are catpured in the data: MHD fluid flows in the compressible limit (sub and super sonic, sub and super Alfvenic).\nHow to evaluate a new simulator operating in this space: Check metrics such as Power spectrum, two-points correlation function.", "id": "Magnetohydrodynamics (MHD) compressible turbulence", "primary_category": "physics.flu-dyn", "authors": "['B. Burkhart',  'S. M. Appel',  'S. Bialy',  'J. Cho',  'A. J. Christensen',  'D. Collins',  'C. Federrath',  'D. B. Fielding',  'D. Finkbeiner',  'A. S. Hill',  'J. C. Ibáñez-Mejía',  'M. R. Krumholz',  'A. Lazarian',  'M. Li',  'P. Mocz',  'M.-M. Mac Low',  'J. Naiman',  'S. K. N. Portillo',  'B. Shane',  'Z. Slepian',  'Y. Yuan']"}
{"dataset": "acoustic_scattering_discontinuous_2d", "url": null, "links": null, "abstract": "These variable-coefficient acoustic equations describe the propogation of an acoustic pressure wave through domains consisting of multiple materials with different scattering properties. This problem emerges in source optimization and it's inverse - that of identifying the material properties from the scattering of the wave - is a vital problem in geology and radar design. This is the simplest of three scenarios. In this case, we have a variable number of initial point sources and single discontinuity separating two sub-domains. Within each subdomain, the density of the underlying material varies smoothly.\nDimension of discretized data: $101$ steps of $256\\times 256$ images.\nFields available in the data: pressure (scalar field), material density (constant scalar field), material speed of sound (constant scalar field), velocity field (vector field). \nNumber of trajectories: 2000.\nEstimated size of the ensemble of all simulations: 157.7 GB.\nGrid type: uniform, cartesian coordinates.\nInitial conditions: Flat pressure static field with 1-4 high pressure rings randomly placed in domain. The rings are defined with variable intensity $\\sim \\mathcal U(.5, 2)$ and radius $\\sim \\mathcal U(.06, .15)$. \nBoundary conditions: Open domain in $y$, reflective walls in $x$.\nSimulation time-step: Variable based on CFL with safety factor .25. \nData are stored separated by ($\\Delta t$): 2/101. \nTotal time range ($t_{min}$ to $t_{max}$): [0, 2.]\nSpatial domain size ($L_x$, $L_y$, $L_z$): [-1, 1] x [-1, 1]\nSet of coefficients or non-dimensional parameters evaluated:\n$K$ is fixed at 4.0. \n$\rho$ is the primary coefficient here. Each side is generated with one of the following distributions:\n- Gaussian Bump - Peak density samples from $\\sim\\mathcal U(1, 7)$ and $\\sigma \\sim\\mathcal U(.1, 5)$ with the center of the bump uniformly sampled from the extent of the subdomain.\n- Linear gradient - Four corners sampled with $\rho \\sim \\mathcal U(1, 7)$. Inner density is bilinearly interpolated.\n- Constant - Constant $\rho \\sim\\mathcal U(1, 7)$\n- Smoothed Gaussian Noise - Constant background sampled $\rho \\sim\\mathcal U(1, 7)$ with IID standard normal noise applied. This is then smoothed by a Gaussian filter of varying sigma $\\sigma \\sim\\mathcal U(5, 10)$\nApproximate time to generate the data: ~15 minutes per simulation. \nHardware used to generate the data and precision used for generating the data: 64 Intel Icelake cores per simulation. Generated in double precision.\nWave propogation through discontinuous media. Most existing machine learning datasets for computational physics are highly smooth and the acoustic challenges presented here offer challenging discontinuous scenarios that approximate complicated geometry through the variable density.", "id": "Acoustic Scattering - Single Discontinuity", "primary_category": "physics.flu-dyn", "authors": ["The Well Collaboration"]}
{"dataset": "acoustic_scattering_inclusions_2d", "url": null, "links": null, "abstract": "These variable-coefficient acoustic equations describe the propogation of an acoustic pressure wave through domains consisting of multiple materials with different scattering properties. This problem emerges in source optimization and it's inverse - that of identifying the material properties from the scattering of the wave - is a vital problem in geology and radar design. In this case, we have a variable number of initial point sources and a domain with random inclusions. These types of problems are of particular interest in geology where the inverse scattering is used to identify mineral deposits.\nDimension of discretized data: $101$ steps of $256\\times256$ images.\nFields available in the data: pressure (scalar field), material density (constant scalar field), material speed of sound (constant scalar field), velocity field (vector field).\nNumber of trajectories: 4000.\nEstimated size of the ensemble of all simulations: 283.8 GB.\nGrid type: uniform, cartesian coordinates.\nInitial conditions: Flat pressure static field with 1-4 high pressure rings randomly placed in domain. The rings are defined with variable intensity $\\sim \\mathcal U(.5, 2)$ and radius $\\sim \\mathcal U(.06, .15)$. \nBoundary conditions: Open domain in $y$, reflective walls in $x$.\nSimulation time-step: Variable based on CFL with safety factor .25. \nData are stored separated by ($\\Delta t$): 2/101. \nTotal time range ($t_{min}$ to $t_{max}$): [0, 2.].\nSpatial domain size ($L_x$, $L_y$, $L_z$): [-1, 1] x [-1, 1].\nSet of coefficients or non-dimensional parameters evaluated:\n$K$ is fixed at 4.0. \n$\rho$ is the primary coefficient here. This is a superset of the single discontinuity example so the background is first generated two splits with one of the following distributions:\n- Gaussian Bump - Peak density samples from $\\sim\\mathcal U(1, 7)$ and $\\sigma \\sim\\mathcal U(.1, 5)$ with the center of the bump uniformly sampled from the extent of the subdomain.\n- Linear gradient - Four corners sampled with $\rho \\sim \\mathcal U(1, 7)$. Inner density is bilinearly interpolated.\n- Constant - Constant $\rho \\sim\\mathcal U(1, 7)$\n- Smoothed Gaussian Noise - Constant background sampled $\rho \\sim\\mathcal U(1, 7)$ with IID standard normal noise applied. This is then smoothed by a Gaussian filter of varying sigma $\\sigma \\sim\\mathcal U(5, 10)$. \nInclusions are then added as 1-15 random ellipsoids with center uniformly sampled from the domain and height/width sampled uniformly from [.05, .6]. The ellipsoid is then rotated randomly with angle sampled [-45, 45]. For the inclusions, $Ln(\rho)\\sim \\mathcal U(-1, 10)$ \nApproximate time to generate the data: ~15 minutes per simulation. \nHardware used to generate the data and precision used for generating the data: 64 Intel Icelake cores per simulation. Generated in double precision.\nWave propogation through discontinuous media. Most existing machine learning datasets for computational physics are highly smooth and the acoustic challenges presented here offer challenging discontinuous scenarios that approximate complicated geometry through the variable density. The inclusions change wave propogation speed but only in small, irregular areas.", "id": "Acoustic Scattering - Inclusions", "primary_category": "physics.flu-dyn", "authors": ["The Well Collaboration"]}
{"dataset": "acoustic_scattering_maze_2d", "url": null, "links": null, "abstract": "These variable-coefficient acoustic equations describe the propogation of an acoustic pressure wave through maze-like domains. Pressure waves emerge from point sources and propogate through domains consisting of low density maze paths and orders of magnitude higher density maze walls. This is built primarily as a challenge for machine learning methods, though has similar properties to optimal placement problems like WiFi in a building.\nDimension of discretized data: $201$ steps of $256\\times256$ images.\nFields available in the data: pressure (scalar field), material density (constant scalar field), material speed of sound (constant scalar field), velocity field (vector field).\nNumber of trajectories: 2000.\nEstimated size of the ensemble of all simulations: 311.3 GB.\nGrid type: uniform, cartesian coordinates.\nInitial conditions: Flat pressure static field with 1-6 high pressure rings randomly placed along paths of maze. The rings are defined with variable intensity $\\sim \\mathcal U(3., 5.)$ and radius $\\sim \\mathcal U(.01, .04)$. Any overlap with walls is removed. \nBoundary conditions: Open domain in $y$, reflective walls in $x$.\nSimulation time-step: Variable based on CFL with safety factor .25. \nData are stored separated by ($\\Delta t$): 2/201. \nTotal time range ($t_{min}$ to $t_{max}$): [0,4.].\nSpatial domain size ($L_x$, $L_y$, $L_z$): [-1, 1] x [-1, 1].\nSet of coefficients or non-dimensional parameters evaluated:\n$K$ is fixed at 4.0. \n$\rho$ is the primary coefficient here. We generated a maze with initial width between 6 and 16 pixels and upsample it via nearest neighbor resampling to create a 256 x 256 maze. The walls are set to $\rho=10^6$ while paths are set to  $\rho=3$.  \nApproximate time to generate the data: ~20 minutes per simulation. \nHardware used to generate the data and precision used for generating the data: 64 Intel Icelake cores per simulation. Generated in double precision.\nThis is an example of simple dynamics in complicated geometry. The sharp discontinuities can be a significant problem for machine learning models, yet they are a common feature in many real-world physics. While visually the walls appear to stop the signal, it is actually simply the case that the speed of sound is much much lower inside the walls leading to partial reflection/absorbtion at the interfaces.", "id": "Acoustic Scattering - Maze", "primary_category": "physics.flu-dyn", "authors": ["The Well Collaboration"]}
{"dataset": "active_matter", "url": null, "links": null, "abstract": "Simulation of a continuum theory describing the dynamics of $N$ rod-like active particles immersed in a Stokes fluid having linear dimension $L$ and colume $L^2$.\nDimension of discretized data: $81$ time-steps of $256\\times256$ images per trajectory.\nFields available in the data: concentration (scalar field),\nvelocity (vector field), orientation tensor (tensor field), strain-rate tensor (tensor field).\nNumber of trajectories: $5$ trajectories per parameter-set, each trajectory being generated with a different initialization of the state field {$c,D,U$}.\nSize of the ensemble of all simulations: 51.3 GB.\nGrid type: Uniform grid, cartesian coordinates.\nInitial conditions: The concentration is set to constant value $c(x,t)=1$ and the orientation tensor is initialized as plane-wave perturbation about the isotropic state.\nBoundary conditions: Periodic boundary conditions.\nSimulation time-step: $3.90625\\times 10^{-4}$ seconds.\nData are stored separated by ($\\Delta t$): 0.25 seconds.\nTotal time range ($t_{min}$ to $t_{max}$): $0$ to $20$ seconds.\nSpatial domain size ($L_x$, $L_y$, $L_z$): $L_x=10$ and $L_y=10$\nSet of coefficients or non-dimensional parameters evaluated: $\u0007lpha =$ {-1,-2,-3,-4,-5}; $\beta  =$ {0.8}; \n$\\zeta =$ {1,3,5,7,9,11,13,15,17}.\nApproximate time and hardware to generate the data: 20 minutes per simulation on an A100 GPU in double precision. There is a total of 225 simulations, which is approximately 75 hours.\nWhat phenomena of physical interest are catpured in the data: How is energy being transferred between scales? How is vorticity coupled to the orientation field? Where does the transition from isotropic state to nematic state occur with the change in alignment ($\\zeta$) or dipole strength ($\u0007lpha$)? \nHow to evaluate a new simulator operating in this space: Reproducing some summary statistics like power spectra and average scalar order parameters. Additionally, being able to accurately capture the phase transition from isotropic to nematic state.", "id": "Active fluid simulations", "primary_category": "physics.flu-dyn", "authors": ["The Well Collaboration"]}
{"dataset": "convective_envelope_rsg", "url": "https://iopscience.iop.org/article/10.3847/1538-4357/ac5ab3", "links": "https://iopscience.iop.org/article/10.3847/1538-4357/ac5ab3", "abstract": "Massive stars evolve into red supergiants, which have large radii and luminosities, and low-density, turbulent, convective envelopes. These simulations model the (inherently 3D) convective properties and gives insight into the progenitors of supernovae explosions.\nDimension of discretized data: $100$ time-steps of $256\\times 128 \\times 256$ images per trajectory.\nFields available in the data: energy (scalar field), density (scalar field), pressure (Scalar field), velocity (vector field).\nNumber of trajectories: 29 (they are cuts of one long trajectory, long trajectory available on demand).\nEstimated size of the ensemble of all simulations: 570 GB.\nGrid type: spherical coordinates, uniform in $(\\log r, \theta,\\phi)$.  Simulations are done for a portion of a sphere (not the whole sphere), so the simulation volume is like a spherical cake slice.\nInitial and boundary conditions: The temperature at the inner boundary (IB) is first set to equal that of the appropriate radius coordinate in the MESA (1D) model ($400\\~R_\\odot$ and $300\\~R_\\odot$) and the density selected to approximately recover the initial total mass of the star in the simulation ($15.4\\~M_\\odot$ and $14\\~M_\\odot$). \nBetween $300\\~R_\\odot$ and $400\\~R_\\odot$, the initial profile is constructed with the radiative luminosity to be $10^5\\~L_\\odot$, and this is kept fixed in the IB.\nSimulation time-step: 198 days.\nData are stored separated by ($\\Delta t$): units here are sort of arbitrary, $\\Delta t= 8$.\nTotal time range ($t_{min}$ to $t_{max}$):  0, 806 (arbitrary).\nSpatial domain size: $R$ from $300-6700~{\rm R_\\odot}$, θ from $π/4−3π/4$ and $\\phi$ from $0−π$, with $δr/r ≈ 0.01$.\nSet of coefficients or non-dimensional parameters evaluated:\n| Simulation | radius of inner boundary $R_{IB}/R_\\odot$ | radius of outer boundary $R_{OB}/R_\\odot$ | heat source | resolution (r × θ × $\\phi$) | duration | core mass $mc/M\\odot$ | final mass $M_{\rm final}/M_\\odot$ |\n|--|--|--|--|--|--|--|--|\n| Whole simulation (to obtain the 29 trajectories) | 300 | 6700 | fixed L | 256 × 128 × 256 | 5766 days | 10.79 | 12.9 |\nApproximate time to generate the data: 2 months on 80 nodes for each run.\nHardware used to generate the data: 80x NASA Pleiades Skylake CPU nodes.\nWhat phenomena of physical interest are captured in the data: turbulence and convection (inherently 3D processes), variability.\nHow to evaluate a new simulator operating in this space: can it predict behaviour of simulation in convective steady-state, given only perhaps a few snapshots at the beginning of the simulation?\nCaveats: complicated geometry, size of a slice in R varies with R (think of this as a slice of cake, where the parts of the slice closer to the outside have more area/volume than the inner parts), simulation reaches convective steady-state at some point and no longer \"evolves\".", "id": "Red Supergiant Convective Envelope", "primary_category": "physics.flu-dyn", "authors": "['Jared A. Goldberg', 'Yan-Fei Jiang', 'Lars Bildsten']"}
{"dataset": "euler_quadrants", "url": "https://epubs.siam.org/doi/pdf/10.1137/S1064827595291819?casa_token=vkASCwD4WngAAAAA:N0jy0Z6tshitF10_YRTlZzU-P7mAiPFr3v58sw7pmRsZOarAi824-b1CWhOQts1rvaG3YpJisw", "links": "https://epubs.siam.org/doi/pdf/10.1137/S1064827595291819?casa_token=vkASCwD4WngAAAAA:N0jy0Z6tshitF10_YRTlZzU-P7mAiPFr3v58sw7pmRsZOarAi824-b1CWhOQts1rvaG3YpJisw", "abstract": "The evolution can give rise to shocks, rarefaction waves, contact discontinuities, interaction with each other and domain walls.\nDimension of discretized data: 100 timesteps of 512x512 images.\nFields available in the data: density (scalar field), energy (scalar field), pressure (scalar field), momentum (vector field).\nNumber of trajectories: 500 per set of parameters, 10 000 in total.\nEstimated size of the ensemble of all simulations: 5.17 TB.\nGrid type: uniform, cartesian coordinates.\nInitial conditions: Randomly generated initial quadrants.\nBoundary conditions: Periodic or open.\nSimulation time-step: variable.\nData are stored separated by ($\\Delta t$): 0.015s (1.5s for 100 timesteps).\nTotal time range ($t_{min}$ to $t_{max}$): $t_{min} = 0$, $t_{max}=1.5s$.\nSpatial domain size ($L_x$, $L_y$, $L_z$): $L_x = 1$ and  $L_y = 1$.\nS**et of coefficients or non-dimensional parameters evaluated: all combinations of $\\gamma$ constant of the gas: $\\gamma=${1.3,1.4,1.13,1.22,1.33,1.76, 1.365,1.404,1.453,1.597} and boundary conditions: {extrap, periodic}.\nApproximate time to generate the data: 80 hours on 160 CPU cores for all data.\nHardware used to generate the data and precision used for generating the data: Icelake nodes, double precision.\nWhat phenomena of physical interest are catpured in the data: capture the shock formations and interactions. Multiscale shocks.\nHow to evaluate a new simulator operating in this space: the new simulator should predict the shock at the right location and time, and the right shock strength, as compared to a ‘pressure’ gauge monitoring the ‘exact’ solution.", "id": "Euler Multi-quadrants - Riemann problems (compressible, inviscid fluid)", "primary_category": "physics.flu-dyn", "authors": "['Peter D. Lax', 'Xu-Dong Liu']"}
{"dataset": "helmholtz_staircase", "url": null, "links": null, "abstract": "Accurate solution of PDEs near infinite, periodic boundaries poses a numerical challenge due these surfaces serving as waveguides, allowing modes to propagate for long distances from the source. This property makes numerical truncation of the (infinite) solution domain unfeasible, as it would induce large artificial reflections and therefore errors. Periodization (reducing the computational domain to one unit cell) is only possible if the incident wave is also\nperiodic, such as plane waves, but not for nonperiodic sources, e.g. a point source. Computing a high-order accurate scattering solution from a point source, however, would be of scientific interest as it models applications such as remote sensing, diffraction from gratings, antennae, or acoustic/photonic metamaterials. We use a combination of the Floquet—Bloch transform (also known as array scanning method) and boundary integral equation methods to alleviate these challenges and recover the scattered solution as an integral over a family of quasiperiodic solutions parameterized by their on-surface wavenumber. The advantage of this approach is that each of the quasiperiodic solutions may be computed quickly by periodization, and accurately via high-order quadrature.\nDimension of discretized data: $50$ time-steps of \n$1024\\times256$ images.\nFields available in the data:\nreal and imaginary part of accoustic pressure (scalar field), the staircase mask (scalar field, stationary).\nNumber of trajectories: $512$ (combinations of $16$ input parameter $\\omega$ and $32$ source positions $\\mathbf{x}_0$).\nSize of the ensemble of all simulations: 52.4 GB.\nGrid type: uniform.\nInitial conditions: The time-dependence is\nanalytic in this case: $U(t, \\mathbf{x}) = u(\\mathbf{x})e^{-i\\omega t}.$ Therefore any spatial solution may serve as an initial condition.\nBoundary conditions: Neumann conditions (normal\nderivative of the pressure $u$ vanishes, with the normal defined as pointing up from\nthe boundary) are enforced at the boundary.\nSimulation time-step: continuous in time (time-dependence is\nanalytic).\nData are stored separated by ($\\Delta t$): $\\Delta t =\frac{2\\pi}{\\omega N}$, with $N = 50$.\nTotal time range ($t_{min}$ to $t_{max}$): $t_{\\mathrm{min}} = 0$, $t_{\\mathrm{max}} =\n\frac{2\\pi}{\\omega}$.\nSpatial domain size ($L_x$, $L_y$, $L_z$): $-8.0 \\leq x_1 \\leq 8.0$ horizontally, and $-0.5 \\geq x_2 \\geq 3.5$ vertically.\nSet of coefficients or non-dimensional parameters evaluated: $\\omega$={0.06283032, 0.25123038, 0.43929689, 0.62675846, 0.81330465, 0.99856671, 1.18207893, 1.36324313, 1.5412579, 1.71501267, 1.88295798, 2.04282969, 2.19133479, 2.32367294, 2.4331094,  2.5110908}, with the sources coordinates being all combinations of $x$={-0.4, -0.3, -0.2, -0.1, 0, 0.1, 0.2, 0.3, 0.4} and $y$={-0.2, -0.1, 0, 0.1, 0.2, 0.3, 0.4}.\nApproximate time to generate the data: per input parameter: $\\sim 400s$, total: $\\sim 50$ hours.\nHardware used to generate the data: 64 CPU cores.\nWhat phenomena of physical interest are captured in the data: The simulations capture the existence of trapped acoustic waves – modes that are guided along the corrugated surface. They also show that the on-surface wavenumber of trapped modes is different than the frequency of the incident radiation, i.e. they capture the trapped modes’ dispersion relation.\nHow to evaluate a new simulator operating in this space:\nThe (spatial) accuracy of a new simulator/method could be checked by requiring that it conserves flux – whatever the source injects into the system also needs to come out. The trapped modes’ dispersion relation may be another metric, my method generates this to 7-8 digits of accuracy at the moment, but 10-12 digits may also be obtained. The time-dependence learnt by a machine learning algorithm can be compared to the analytic solution $e^{-i\\omega t}$, this can be used to evaluate temporal accuracy.", "id": "Helmholtz equation on a 2D staircase", "primary_category": "physics.flu-dyn", "authors": ["The Well Collaboration"]}
{"dataset": "pattern_formation", "url": null, "links": null, "abstract": "The Gray-Scott equations are a set of coupled reaction-diffusion equations describing two chemical species, $A$ and $B$, whose concentrations vary in space and time. The two parameters $f$ and $k$ control the “feed” and “kill” rates in the reaction. A zoo of qualitatively different static and dynamic patterns in the solutions are possible depending on these two parameters. There is a rich landscape of pattern formation hidden in these equations.\nDimension of discretized data: 1001 time-steps of $128\\times 128$ images.\nFields available in the data: Two chemical species $A$ and $B$.\nNumber of trajectories: 6 sets of parameters, 200 initial conditions per set = 1200.\nEstimated size of the ensemble of all simulations: 153.8 GB.\nGrid type: uniform, cartesian coordinates.\nInitial conditions: Two types of initial conditions generated: random Fourier series and random clusters of Gaussians.\nBoundary conditions: periodic.\nSimulation time-step: 1 second.\nData are stored separated by ($\\Delta t$): 10 seconds.\nTotal time range ($t_{min}$ to $t_{max}$): $t_{min} =0$, $t_{max} = 10,000$.\nSpatial domain size ($L_x$, $L_y$, $L_z$): $x,y\\in[-1,1]$.\nSet of coefficients or non-dimensional parameters evaluated: All simulations used $\\delta_u = 2.10^{-5}$ and $\\delta_v = 1.10^{-5}$.\n\"Gliders\": $f = 0.014, k = 0.054$. \"Bubbles\": $f = 0.098, k =0.057$. \"Maze\": $f= 0.029, k = 0.057$. \"Worms\": $f= 0.058, k = 0.065$. \"Spirals\": $f=0.018, k = 0.051$. \"Spots\": $f= 0.03, k=0.062$.\nApproximate time to generate the data: 5.5 hours per set of parameters, 33 hours total.\nHardware used to generate the data: 40 CPU cores.\nWhat phenomena of physical interest are catpured in the data: Pattern formation: by sweeping the two parameters $f$ and $k$, a multitude of steady and dynamic patterns can form from random initial conditions.\nHow to evaluate a new simulator operating in this space: It would be impressive if a simulator—trained only on some of the patterns produced by a subset of the $(f, k)$ parameter space—could perform well on an unseen set of parameter values $(f, k)$ that produce fundamentally different patterns. Stability for steady-state patterns over long rollout times would also be impressive.", "id": "Pattern formation in the Gray-Scott reaciton-diffusion equations", "primary_category": "physics.flu-dyn", "authors": ["The Well Collaboration"]}
{"dataset": "planetswe", "url": "https://openreview.net/forum?id=RFfUUtKYOG", "links": "https://openreview.net/forum?id=RFfUUtKYOG", "abstract": "The shallow water equations are fundamentally a 2D approximation of a 3D flow in the case where horizontal length scales are significantly longer than vertical length scales. They are derived from depth-integrating the incompressible Navier-Stokes equations. The integrated dimension then only remains in the equation as a variable describing the height of the pressure surface above the flow. These equations have long been used as a simpler approximation of the primitive equations in atmospheric modeling of a single pressure level, most famously in the Williamson test problems. This scenario can be seen as similar to Williamson Problem 7 as we derive initial conditions from the hPa 500 pressure level in ERA5. These are then simulated with realistic topography and two levels of periodicity.\nDimension of discretized data: 3024 timesteps of 256x512 images with \"day\" defined as 24 steps and \"year\" defined as 1008 in model time. \nFields available in the data: height (scalar field), velocity (vector field).\nNumber of trajectories: 40 trajectories of 3 model years.\nEstimated size of the ensemble of all simulations: 185.8 GB.\nGrid type: Equiangular grid, polar coordinates.\nInitial conditions: Sampled from hPa 500 level of [ERA5](https://rmets.onlinelibrary.wiley.com/doi/10.1002/qj.3803), filtered for stable initialization and burned-in for half a simulation year. \nBoundary conditions: Spherical.\nSimulation time-step ($\\Delta t$): CFL-based step size with safety factor of .4. \nData are stored separated by ($\\delta t$): 1 hour in simulation time units.\nTotal time range ($t_{min}$ to $t_{max}$): $t_{min} = 0$, $t_{max} = 3024$.\nSpatial domain size ($L_x$, $L_y$, $L_z$): $\\phi \\in [0, 2 \\pi]$, $\theta \\in [0, \\pi]$. \nSet of coefficients or non-dimensional parameters evaluated: $\nu$ normalized to mode 224. \nApproximate time to generate the data: 45 minutes using 64 icelake cores for one simulation. \nHardware used to generate the data: 64 Icelake CPU cores.\nSpherical geometry and planet-like topography and forcing make for a proxy for real-world atmospheric dynamics where true dynamics are known. The dataset has annual and daily periodicity forcing models to either process a sufficient context length to learn these patterns or to be explicitly time aware. Furthermore, the system becomes stable making this a good system for exploring long run stability of models.", "id": "PlanetSWE", "primary_category": "physics.flu-dyn", "authors": "['Michael McCabe', 'Peter Harrington', 'Shashank Subramanian', 'Jed Brown']"}
{"dataset": "post_neutron_star_merger", "url": null, "links": null, "abstract": "The simulations presented here are axisymmetrized snapshots of full three-dimensional general relativistic neutrino radiation magnetohydrodynamics. The plasma physics is treated with finite volumes with constrained transport for the magnetic field on a curvilinear grid. The system is closed by a tabulated nuclear equation of state assuming nuclear statistical equilibrium (NSE). The radiation field is treated via Monte Carlo transport, which is a particle method. The particles are not included in this dataset, however their effects are visible as source terms on the fluid.\nDimension of discretized data: 181 time-steps of $192 \\times 128 \\times 66$ snapshots.\nFields available in the data: fluid density (scalar field), fluid internal energy (scalar field), electron fraction (scalar field), temperate (scalar field), entropy (scalar field), velocity (vector field), magnetic field (vector field), contravariant tensor metric of space-time (tensor field, no time-dependency).\nA description of fields available in an output file can be found here:\nhttps://github.com/lanl/nubhlight/wiki\nNumber of trajectories: Currently eight full simulations. \nSize of the ensemble of all simulations: 110.1 GB.\nGrid type**: Uniform grid, log-spherical coordinates.\nInitial conditions: Constant entropy torus in hydrostatic equilibrium orbiting a black hole. Black hole mass and spin, as well as torus mass, spin, electron fraction, and entropy vary.\nBoundary conditions: open.\nSimulation time-step: approximately 0.01 in code units. Physical time varies; roughly 147 nanoseconds for fiducial model.\nData are stored separated by ($\\Delta t$): 50 in code units. Physical time varies; roughly 0.6 milliseconds for fiducial model.\nTotal time range ($t_{min}$ to $t_{max}$): 10000 in code units. Physical time varies; roughly 127 milliseocnds for fudicial model\nSpatial domain size ($L_x$, $L_y$, $L_z$): Spherical coordinates. Radius roughly 2 to 1000 in code units. Physical values vary. Outer boundary is at roughly 4000 for fiducial model. Polar angle 0 to pi. Azimuthal angle 0 to 2*pi. Note that the coordinates are curvilinear. In Cartesian space, spacing is logarithmic in radius and there is a focusing of grid lines near the equator.\nSet of coefficients or non-dimensional parameters evaluated: Black hole spin parameter a, ranges 0 to 1. Initial mass and angular momentum of torus. In dimensionless units, evaluated as inner radius Rin and radius of maximum pressure Rmax. Torus initial electron fraction Ye and entropy kb. Black hole mass in solar masses.\nApproximate time to generate the data: Roughly 3 weeks per simulation on 300 cores.\nHardware used to generate the data and precision used for generating the data: Data generated at double precision on several different supercomputers. All calculations were CPU calculations parallelized with a hybrid MPI + OpenMP strategy. 1 MPI rank per socket. Oldest calculations performed on the Los Alamos Badger cluster, now decommissioned. Intel Xeon E5-2695v5 2.1 GHz. 12 cores per socket, 24 core cores per node. Simulations run on 33 nodes. Some newer simulations run on Los Alamos Capulin cluster, now decomissioned. ARM ThunderX2 nodes. 56 cores per node. Simulation run on 33 nodes.\n## Simulation Index\n| Scenario | Shorthand name | Description                                                         |\n|----------|----------------|---------------------------------------------------------------------|\n| 0        | collapsar_hi   | Disk resulting from collapse of massive rapidly rotating star.       |\n| 1        | torus_b10      | Disk inspired by 2017 observation of a neutron star merger. Highest magnetic field strength. |\n| 2        | torus_b30      | Disk inspired by 2017 observation of a neutron star merger. Intermediate magnetic field strength. |\n| 3        | torus_gw170817 | Disk inspired by 2017 observation of a neutron star merger. Weakest magnetic field strength. |\n| 4        | torus_MBH_10   | Disk from black hole-neutron star merger. 10 solar mass black hole.  |\n| 5        | torus_MBH_2p31 | Disk from black hole-neutron star merger. 2.31 solar mass black hole.|\n| 6        | torus_MBH_2p67 | Disk from black hole-neutron star merger. 2.76 solar mass black hole.|\n| 7        | torus_MBH_2p69 | Disk from black hole-neutron star merger. 2.79 solar mass black hole.|\n| 8        | torus_MBH_6    | Disk from black hole-neutron star merger. 6 solar mass black hole.   |\n## General relativistic quantities\nThe core quantity that describes the curvature of spacetime and its\nimpact on a simulation is `['t0_fields']['gcon']` of the HDF5 file. From this other\nquantities can be computed.\n## To reproduce\nThe values in `simulation_parameters.json` are sufficient to reproduce a\nsimulation using [nubhlight](https://github.com/lanl/nubhlight) using\nthe `torus_cbc` problem generator, with one exception. You must\nprovide tabulated equation of state and opacity data. We use the SFHo\nequation of state provided on the\n[stellar collapse website](https://stellarcollapse.org/).\nTabulated neutrino opacities were originally computed for the Fornax\ncode and are not public. However adequate open source substitutes may\nbe generated by the [nulib](http://www.nulib.org/) library.\n## Explanation of simulation parameters\nHere we include, for completeness, a description of the different simulation parameters. which cover the simulation parameters chosen. Their value for each simulation is stored in `simulation_parameters.json`.\n- `B_unit`, the unit of magnetic field strength. Multiplying code quantity by `B_unit` converts the quantity to units of Gauss.\n- `DTd`, dump time cadence.\n- `DTl`, log output time cadence.\n- `DTp`, permanent restart file time cadence.\n- `DTr`, temporary restart file time cadence.\n- `Ledd`, (Photon) Eddington luminosity based on black hole mass.\n- `L_unit`, length unit. Multiplying code quantity by `L_unit` converts it into units of cm.\n- `M_unit`, mass unit. Multiplying code quantity by `M_unit` converts it into units of g.\n- `Mbh`, black hole mass in units of g.\n- `MdotEdd`, (Photon) Eddington accretion rate based on black hole mass.\n- `N1`, number of grid points in X1 (radial) direction.\n- `N2`, number of grid points in X2 (polar) direction.\n- `N3`, number of grid points in X3 (azimuthal) direction.\n- `PATH`, output directory for the original simulation.\n- `RHO_unit`, density unit. Multiplying code quantity by `RHO_unit` converts it into units of g/cm^3.\n- `Reh`, radius of the event horizon in code units.\n- `Rin`, radius of the inner boundary in code units.\n- `Risco`, radius of the innermost stable circular orbit in code units.\n- `Rout_rad`, outer radius of neutrino transport.\n- `Rout_vis`, radius used for 3D volume rendering.\n- `TEMP_unit`, temperature unit. Converts from MeV (code units) to Kelvin.\n- `T_unit`, time unit. Converts from code units to seconds.\n- `U_unit`, energy density unit. Multiplying code quantity by `U_unit` converts it into units of erg/cm^3.\n- `a`, dimensionless black hole spin. \n- `cour`, dimensionless CFL factor used to set the timestep based on the grid spacing.\n- `dx`, array of grid spacing in code coordinates. (Uniform.)\n- `maxnscatt`, maximum number of scattering events per superphoton particle\n- `mbh`, black hole mass in solar masses.\n- `hslope`, `mks_smooth`, `poly_alpha`, `poly_xt` focusing terms used for coordinate transforms\n- `startx`, array of starting coordinate values for `X1`,`X2`,`X3` in code coordinates.\n- `stopx`, array of ending coordinate values for `X1`,`X2`,`X3` in code coordinates.\n- `tf`, final simulation time.\n- `variables` list of names of primitive state vector.\nWhat phenomena of physical interest are catpured in the data: The 2017 detection of the in-spiral and merger of two neutron stars\nwas a landmark discovery in astrophysics. Through a wealth of\nmulti-messenger data, we now know that the merger of these\nultracompact stellar remnants is a central engine of short gamma ray\nbursts and a site of r-process nucleosynthesis, where the heaviest\nelements in our universe are formed. The radioactive decay of unstable\nheavy elements produced in such mergers powers an optical and\ninfra-red transient: The kilonova.\nOne key driver of nucleosynthesis and resultant electromagnetic\nafterglow is wind driven by an accretion disk formed around the\ncompact remnant. Neutrino transport plays a key role in setting the\nelectron fraction in this outflow, thus controlling the\nnucleosynthesis.\nCollapsars are black hole accretion disks formed after the core of a\nmassive, rapidly rotating star collapses to a black hole. These\ndramatic systems rely on much the same physics and modeling as\npost-merger disks, and can also be a key driver of r-processes\nnucleosynthesis.\nHow to evaluate a new simulator operating in this space: The electron fraction of material blown off from the disk is the core\n\"delivarable.\" It determines how heavy elements are synthesized, which\nin turn determines the electromagnetic counterpart as observed on\nEarth. This is the most important piece to get right from an emulator.", "id": "Post neutron star merger", "primary_category": "physics.flu-dyn", "authors": ["The Well Collaboration"]}
{"dataset": "rayleigh_benard", "url": null, "links": null, "abstract": "Rayleigh-Bénard convection involves fluid dynamics and thermodynamics, seen in a horizontal fluid layer heated from below, forming convective cells due to a temperature gradient. With the lower plate heated and the upper cooled, thermal energy creates density variations, initiating fluid motion. This results in Bénard cells, showcasing warm fluid rising and cool fluid descending. The interplay of buoyancy, conduction, and viscosity leads to complex fluid motion, including vortices and boundary layers.\nDimension of discretized data: $200$ timesteps of \n$512\\times128$ images.\nFields are available in the data: buoyancy (scalar vield), pressure (scalar field), velocity (vector field).\nNumber of simulations: $1750$ ($35$ PDE parameters $\\times$ $50$ initial conditions).\nSize of the ensemble of all simulations: 358.4 GB.\nGrid type: uniform, cartesian coordinates.\nInitial conditions: the buoyancy is composed of a dumped noise added to a linear background  $b(t=0) = (Lz-z)\\times\\delta b_0 + z(Lz-z) \\times\\epsilon$ where $\\epsilon$ is a Gaussian white noise of scale $10^{-3}$.\nThe other fields $u$ and $p$ are initialized to $0$.\nBoundary conditions: periodic on the horizontal direction, Dirichlet conditions on the vertical direction.\nSimulation time-step: 0.25.\nTotal time range ($t_{min}$ to $t_{max}$): $t_{min} = 0$, $t_{max} = 50$.\nSpatial domain size: $0 \\leq x \\leq 4$ horizontally, and $0 \\leq z \\leq 1$ vertically.\nSet of coefficients or non-dimensional parameters evaluated: $\text{Rayleigh}\\in[1e6,1e7,1e8,1e9,1e10], \text{Prandtl}\\in[0.1,0.2,0.5,1.0,2.0,5.0,10.0]$. For initial conditions $\\delta b_0\\in[0.2,0.4,0.6,0.8,1.0]$, the seed used to generate the initial Gaussian white noise are $40,\\ldots,49$.\nApproximate time to generate the data: per input parameter** from $\\sim6\\,000s$ to $\\sim 50\\,000s$ (high Rayleigh numbers take longer), total: $\\sim 60$ hours.\nHardware used to generate the data and precision used for generating the data: 12 nodes of 64 CPU cores with 8 processes per node, in single precision.\nRayleigh-Bénard convection datasets offer valuable insights into fluid dynamics under thermal gradients, revealing phenomena like thermal plumes and turbulent eddies. Understanding these dynamics is crucial for applications in engineering and environmental science.", "id": "Rayleigh Bénard convection", "primary_category": "physics.flu-dyn", "authors": ["The Well Collaboration"]}
{"dataset": "rayleigh_taylor_instability", "url": "https://www.researchgate.net/publication/243660629_Reynolds_number_effects_on_Rayleigh-Taylor_instability_with_possible_implications_for_type_Ia_supernovae", "links": "https://www.researchgate.net/publication/243660629_Reynolds_number_effects_on_Rayleigh-Taylor_instability_with_possible_implications_for_type_Ia_supernovae", "abstract": "We consider the Rayleigh-Taylor instability for a range of Atwood numbers and initial perturbations, all of which have a log—normal horizontal energy spectrum with random phase. The dataset examines how varying the mean, standard deviation and the disparity of the random phase effects the transition to and statistics of the ensuing turbulent flow.\nDimension of discretized data: 60 time-steps of $128\\times 128\\times 128$ cubes.\nFields available in the data: Density (scalar field), velocity (vector field).\nNumber of trajectories: 45 trajectories.\nEstimated size of the ensemble of all simulations: 255.6 GB.\nGrid type: uniform grid, cartesian coordinates.\nInitial conditions: Initial conditions have been set by imposing a log—normal profile for the shape of energy spectrum in wavenumber space, such that:\n$$A(k) = \frac{1}{k\\sigma\\sqrt{2\\pi}} \\exp\\Big(-\frac{(\\ln (k) - \\mu)^2}{2\\sigma^2}\\Big) \\quad\textrm{with}\\quad k = \\sqrt{k^2_x+k^2_y}$$\nwhere $\\mu$ is the mean and $\\sigma$ is the standard deviation of the profile. Furthermore, we have imposed a random phase to the corresponding complex Fourier component (i.e. a random value for the argument of the complex Fourier component) between zero and a varied maximum ($\\phi_{max}$), finally after Fourier transforming to physical space the mean of the resulting profile is normalized to $3.10^5$ to ensure comparable power. \nBoundary conditions:** Periodic boundary conditions on sides walls and slip conditions on the top and bottom walls.\nSimulation time-step: $\\Delta t$ is set such that the maximum Courant number is $\frac12(CFL_{max}=0.5)$. Therefore, the time step decreases as the flow accelerates.\nData are stored separated by ($\\Delta t$):  different according to At number.\nThe time difference between frames varies as the flow accelerates, thus the largest occur at the beginning of the simulation ($\\delta t \\sim 5s$) and the smallest at the end ($\\delta t \\sim 0.1s$).\nTotal time range ($t_{min}$ to $t_{max}$): Varies from $t_{min}=0$ to $t_{max}$ between $\\sim 30s$ and $\\sim 100s$, depending on Atwood number.\nSpatial domain size ($L_x$, $L_y$, $L_z$): $[0,1]\\times[0,1]\\times[0,1]$.\nSet of coefficients or non-dimensional parameters evaluated: We run simulations with 13 different initializations for five different Atwood number $At\\in {\frac34, \frac12, \frac14, \frac18, \frac{1}{16}}$. The first set on initial conditions considers varying the mean $\\mu$ and standard deviation $\\sigma$ of the profile $A(k)$ with $\\mu\\in{1, 4, 16}$ and $\\sigma\\in{\frac14, \frac12, 1}$, the phase (argument of the complex Fourier component) $\\phi$ was set randomly in the range $[0,2\\pi)$. The second set of initial conditions considers a fixed mean ($\\mu=16$) and standard deviation ($\\sigma =0.25$) and a varieed range of random phases (complex arguments $\\phi\\in[0,\\phi_{max}$)) given to each Fourier component. The four cases considered are specified by $\\phi_{max}\\in { \frac{\\pi}{128}, \frac{\\pi}{8}, \frac{\\pi}{2}, \\pi}$. \nApproximate time to generate the data: 1 hour on 128 CPU cores for 1 simulation. 65 hours on 128 CPU cores for all simulations.\nHardware used to generate the data: 128 CPU core on the Ocre supercomputer at CEA, Bruyères-le-Châtel, France.\nWhat phenomena of physical interest are catpured in the data: In this data there are three key aspects of physical interest. Firstly, impact of coherence on otherwise random initial conditions. Secondly, the effect of the shape of the initial energy spectrum on the structure of the flow. Finally, the transition from the Boussinesq to the non-Boussinesq regime where the mixing width transitions from symmetric to asymmetric growth.  \nHow to evaluate a new simulator operating in this space:\nFrom a fundamental standpoint we, would expect the density field to be advected and mixed rather than created or destroyed to give appropriate statistics. From a qualitative perspective, given that the underlying simulations are of comparable spatial resolution to the simulations run by the alpha group (Dimonte et. al. 2003) we would consider a good emulator to produce a comparable value for α as reported in their paper for an appropriately similar set of initial conditions. This parameter is derived by considering the flow after the initial transient. At this stage, the width of the turbulent mixing zone, $L$, is self-similar and grows as $L= \u0007lpha \\* At \\* g \\* t^2$. They reported a value of $\u0007lpha$=0.025±0.003. In addition, during this self-regime, we would expect to observe energy spectra with a similar shape to those reported in Cabot and Cook 2006, specifically exhibiting an appropriate $k^{-\frac53}$ cascade. From a structural perspective, we would expect that for an initialization with a large variety of modes in the initial spectrum to observe a range of bubbles and spikes (upward and downward moving structures), whereas in the other limit (where this only on mode in the initial spectrum) we expect to observe a single bubble and spike.  In addition, a good emulator would exhibit symmetric mixing with for low Atwood numbers in the Boussinesq regime (defined as $At$ < 0.1 by Andrews and Dalziel 2010) and asymmetries in the mixing with for large Atwood number.", "id": "Rayleigh-Taylor instability", "primary_category": "physics.flu-dyn", "authors": ["William  H. Cabot", "Andrew W. Cook"]}
{"dataset": "shear_flow", "url": null, "links": null, "abstract": "Shear flow are a type of fluid characterized by the continuous deformation of adjacent fluid layers sliding past each other with different velocities. This phenomenon is commonly observed in various natural and engineered systems, such as rivers, atmospheric boundary layers, and industrial processes involving fluid transport.\nThe dataset explores a 2D periodic shearflow governed by incompressible Navier-Stokes equation.\nDimension of discretized data: $200$ time-steps of $128\\times256$ images.\nFields available in the data: tracer (scalar field), velocity (vector field), pressure (scalar field).\nNumber of simulations: $1120$ ($28$ PDE parameters $\\times$ $40$ initial conditions).\nSize of the ensemble of all simulations: 114.7 GB.\nGrid type: uniform, cartesian coordinates.\nInitial conditions: the shear field $u_1$ is composed of $n_\text{shear}$ shears uniformly spaced along the $z$ direction. Each shear is implemented with a tanh (hyperbolic tangent) $\text{tanh}(5\frac{y-y_k}{n_\text{shear}w})$ where $z_k$ is the vertical position of the shear and $w$ is a width factor.\nThe velocity field $u_2$ is composed of sinusoids along the $x$ direction located at the shear. These sinusoids have an exponential decay away from the shear in the $y$ direction $\text{sin}(n_\text{blobs}\\pi x)\\,e^{\frac{25}{w^2}|y-y_k|^2}$.\nThe tracer matches the shear at initialization. The pressure is initialized to zero.\nThe initial condition is thus indexed by $n_\text{shear},n_\text{blobs},w$.\nBoundary conditions: periodic.\nSimulation time-step: 0.1.\nTotal time range ($t_{min}$ to $t_{max}$): $t_{min} = 0$, $t_{max} = 20$.\nSpatial domain size: $0\\leq x \\leq 1$ horizontally, and $-1 \\leq y \\leq 1$ vertically.\nSet of coefficients or non-dimensional parameters evaluated: $\text{Reynolds}\\in[1e4, 5e4, 1e5, 5e5], \text{Schmidt}\\in[0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0]$. For initial conditions $n_\text{shear}\\in[2,4]$, $n_\text{blobs}\\in[2,3,4,5]$, $w\\in[0.25, 0.5, 1.0, 2.0, 4.0]$.\nApproximate time to generate the data: per input parameter: $\\sim 1500s$, total: $\\sim 5$ hours.\nHardware used to generate the data and precision used for generating the data: 7 nodes of 64 CPU cores each with 32 tasks running in parallel on each node, in single precision.\nShear flow are non-linear phenomena arrising in fluid mechanics and turbulence.\nPredicting the behavior of the shear flow under different Reynolds and Schmidt numbers is essential for a number of applications in aerodynamics, automotive, biomedical. \nFurthermore, such flow are unstable at large Reynolds number.", "id": "Periodic shear flow", "primary_category": "physics.flu-dyn", "authors": ["The Well Collaboration"]}
{"dataset": "supernova_explosion_128", "url": null, "links": null, "abstract": "The simulations solve an explosion inside a compression of a monatomic ideal gas, which follows the equation of state with the specific heat ratio $\\gamma=5/3$.\nThe gas in these simulations mocks interstellar medium in the Milky Way Galaxy.\nAt the beginning of the simulations, the thermal energy of a supernova is dumped at the center of the simulation box.\nThe hot ($\\sim 10^7$ K) gas is immediately accelerated and makes the blastwave.\nBecause velocities of the hot gas become supersonic, much fine resolution and small timestep are required to resolve the dynamics.\nThe physical quantities are also distributed in seven orders of magnitude, which requires a large number of simulation steps.\nDimension of discretized data** $59$ time-steps of  $128\\times 128\\times 128$ cubes.\nFields available in the data:\nPressure (scalar field), density (scalar field), temperature(scalar field), velocity (tensor field).\nNumber of trajectories: 260.\nEstimated size of the ensemble of all simulations: 754 GB\nGrid type: uniform, cartesian coordinates.\nInitial conditions: $820$ random seeds generated using https://github.com/amusecode/amuse/blob/main/src/amuse/ext/molecular_cloud.py (Virialized isothermal gas sphere with turbulence following the velocity spectrum $E(k) \\propto k^{-2}$, which is Burgers turbulence ([Burgers 1948](https://www.sciencedirect.com/science/article/abs/pii/S0065215608701005) and [Kupilas+2021](https://doi.org/10.1093/mnras/staa3889) for reference ))\nBoundary conditions: open.\nData are stored separated by ($\\Delta t$): $100$ ~ $10 000$ years (variable timesteps).\nTotal time range ($t_{min}$ to $t_{max}$): $0$ yr to $0.2$ Myr.\nSpatial domain size ($L_x$, $L_y$, $L_z$): 60 pc.\nSet of coefficients or non-dimensional parameters evaluated: Initial temperature $T_0$=\\{100K\\}, Initial number density of hydrogen $\rho_0=$\\{44.5/cc\\}, metallicity (effectively strength of cooling) $Z=\\{Z_0\\}$.\nApproximate time to generate the data (CPU hours):\n|  0.1 M $\\odot$ |\n|:----------:|\n|  $3500$ |\nHardware used to generate the data and precision used for generating the data: up to 1040 CPU cores per run.\nWhat phenomena of physical interest are catpured in the data:\nThe simulations are designed as an supernova explosion, which is the explosion at the last moment of massive stars, in a high-density starforming molecular cloud with a large density contrast. An adiabatic compression of a monatomic ideal gas is assumed.\nTo mimic the explosion, the huge thermal energy ($10^{51}$ erg) is injected at the center of the calculation box and going to make the blastwave, which sweeps out the ambient gas and shells called as supernova feedback. These interactions between supernovae and surrounding gas are interesting because stars are formed in dense and cold regions.\nHowever, calclatig the propagation of blastwaves requires tiny timesteps to calculate and numerous integration steps. When supernova feedback is incorporated in a galaxy simulation, some functions fitted using local high resolution simulations have been used.\nHow to evaluate a new simulator operating in this space:\nIn context of galaxy simulations, the time evolution of thermal energy and momentum are important. We note that those physical quantities are not necessarily conserved because radiative cooling and heating are considered and thermal energy is seamlessly being converted into momentum.", "id": "Supernova Explosion in Turbulent Interstellar medium in galaxies", "primary_category": "physics.flu-dyn", "authors": ["The Well Collaboration"]}
{"dataset": "supernova_explosion_64", "url": null, "links": null, "abstract": "The simulations solve an explosion inside a compression of a monatomic ideal gas, which follows the equation of state with the specific heat ratio $\\gamma=5/3$.\nThe gas in these simulations mocks interstellar medium in the Milky Way Galaxy.\nAt the beginning of the simulations, the thermal energy of a supernova is dumped at the center of the simulation box.\nThe hot ($\\sim 10^7$ K) gas is immediately accelerated and makes the blastwave.\nBecause velocities of the hot gas become supersonic, much fine resolution and small timestep are required to resolve the dynamics.\nThe physical quantities are also distributed in seven orders of magnitude, which requires a large number of simulation steps.\nDimension of discretized data** $59$ time-steps of  $64\\times 64\\times 64$ cubes.\nFields available in the data:\nPressure (scalar field), density (scalar field), temperature(scalar field), velocity (tensor field).\nNumber of trajectories: 740.\nEstimated size of the ensemble of all simulations: 268.2 GB.\nGrid type: uniform, cartesian coordinates.\nInitial conditions: $820$ random seeds generated using https://github.com/amusecode/amuse/blob/main/src/amuse/ext/molecular_cloud.py (Virialized isothermal gas sphere with turbulence following the velocity spectrum $E(k) \\propto k^{-2}$, which is Burgers turbulence ([Burgers 1948](https://www.sciencedirect.com/science/article/abs/pii/S0065215608701005) and [Kupilas+2021](https://doi.org/10.1093/mnras/staa3889) for reference).\nBoundary conditions: open.\nData are stored separated by ($\\Delta t$): $100$ ~ $10,000$ years (variable timesteps). [CHECK]\nTotal time range ($t_{min}$ to $t_{max}$): $0$ yr to $0.2$ Myr.\nSpatial domain size ($L_x$, $L_y$, $L_z$): 60 pc.\nSet of coefficients or non-dimensional parameters evaluated: Initial temperature $T_0$=\\{100K\\}, Initial number density of hydrogen $\rho_0=$\\{44.5/cc\\}, metallicity (effectively strength of cooling) $Z=\\{Z_0\\}$.\nApproximate time to generate the data (CPU hours):\n| 1M $_\\odot$ | 0.1 M $\\odot$ |\n|:----------:|:----------:|\n| $300$ | $3500$ |\nHardware used to generate the data and precision used for generating the data: up to 1040 CPU cores per run.\nWhat phenomena of physical interest are catpured in the data:\nThe simulations are designed as an supernova explosion, which is the explosion at the last moment of massive stars, in a high-density starforming molecular cloud with a large density contrast. An adiabatic compression of a monatomic ideal gas is assumed.\nTo mimic the explosion, the huge thermal energy ($10^{51}$ erg) is injected at the center of the calculation box and going to make the blastwave, which sweeps out the ambient gas and shells called as supernova feedback. These interactions between supernovae and surrounding gas are interesting because stars are formed in dense and cold regions.\nHowever, calclating the propagation of blastwaves requires tiny timesteps to calculate and numerous integration steps. When supernova feedback is incorporated in a galaxy simulation, some functions fitted using local high resolution simulations have been used.\nHow to evaluate a new simulator operating in this space:\nIn context of galaxy simulations, the time evolution of thermal energy and momentum are important. We note that those physical quantities are not necessarily conserved because radiative cooling and heating are considered and thermal energy is seamlessly being converted into momentum.", "id": "Supernova Explosion in Turbulent Interstellar medium in galaxies", "primary_category": "physics.flu-dyn", "authors": ["The Well Collaboration"]}
{"dataset": "turbulence_gravity_cooling", "url": "https://academic.oup.com/mnras/article/526/3/4054/7316686", "links": "https://academic.oup.com/mnras/article/526/3/4054/7316686", "abstract": "These simulations are a turbulent fluid with gravity modeling interstellar medium in galaxies. These fluids make dense filaments, which will form new stars. The timescale and frequency of making new filaments are varied depending on the strength of cooling. It is parametrized by the amount of metal (metallicity), density, and temperature.\nDimension of discretized data: $50$ time-steps of  $64\\times 64\\times 64$ cubes.\nFields available in the data: Pressure (scalar field), density (scalar field), temperature(scalar field), velocity (tensor field).\nNumber of trajectories: 2700 (27 parameters sets $\\times$ 100 runs).\nEstimated size of the ensemble of all simulations: 829.4 GB.\nGrid type: uniform, cartesian coordinates.\nInitial conditions: $2700$ random seeds generated using https://github.com/amusecode/amuse/blob/main/src/amuse/ext/molecular_cloud.py (Virialized isothermal gas sphere with turbulence following the velocity spectrum $E(k) \\propto k^{-2}$, which is Burgers turbulence (Burgers 1948 and Kupilas+2021 for reference )).\nBoundary conditions: open.\nSimulation time-step: $2,000$ ~ $10,000$ years (variable timesteps).\nData are stored separated by ($\\Delta t$): 0.02 free fall time.\nTotal time range ($t_{min}$ to $t_{max}$): 1 Free Fall time (= $L^3/GM$ ); $L=(\rho / \rho_0)^{1/3} \\times 60$ pc, $\rho_0=44.5/\rm{cc}$, $M=1,000,000$ M $_\\odot$.\nSpatial domain size ($L_x$, $L_y$, $L_z$):\n|           | Domain Length ($L$) | Free Fall Time | Snapshot ($\\delta t$) |\n|----------|:----------:|:----------:|:----------:|\n| **Dense (44.5 cm $^{-3}$)** | 60 pc | 6.93 Myr | 0.14 Myr |\n| **Moderate (4.45 cm $^{-3}$)** | 129 pc | 21.9 Myr |0.44 Myr |\n| **Sparse (0.445 cm $^{-3}$)** | 278 pc | 69.3 Myr | 1.4 Myr |\nSet of coefficients or non-dimensional parameters evaluated: Initial temperature $T_0$=\\{10K, 100K, 1000K\\}, Initial number density of hydrogen $\rho_0=$\\{44.5/cc, 4.45/cc, 0.445/cc\\}, metallicity (effectively strength of cooling) $Z=\\{Z_0, 0.1Z_0, 0\\}$.\nApproximate time to generate the data: $600,000$ node hours for all simulations.\n#### For dense dataset (CPU hours)\n|           | Strong (1Z $_\\odot$) | Weak (0.1 Z $_\\odot$) | Adiabatic (0 Z $_\\odot$) |\n|----------:|----------:|----------:|----------:|\n| **$10$ K** | $240$  | $167$ | $77$ |\n| **$100$ K** | $453$ | $204$  | $84$ |\n| **$1000$ K** | $933$ | $186$  | $46$ |\n#### For moderate dataset (CPU hours)\n|           | Strong (1Z $_\\odot$) | Weak (0.1 Z $_\\odot$) | Adiabatic (0 Z $_\\odot$) |\n|----------:|----------:|----------:|----------:|\n| **$10$ K** | $214$  | $75$ | $62$ |\n| **$100$ K** | $556$ | $138$  | $116$ |\n| **$1000$ K** | $442$ | $208$  | $82$ |\n#### For sparse dataset (CPU hours)\n|           | Strong (1Z $_\\odot$) | Weak (0.1 Z $_\\odot$) | Adiabatic (0 Z $_\\odot$) |\n|----------:|----------:|----------:|----------:|\n| **$10$ K** | $187$  | $102$ | $110$ |\n| **$100$ K** | $620$ | $101$  | $92$ |\n| **$1000$ K** | $286$ | $129$  | $93$ |\nHardware used to generate the data and precision used for generating the data: up to 1040 CPU cores per run.\nWhat phenomena of physical interest are catpured in the data:\nGravity, hydrodynamics and radiative cooling/heating are considered in the simulations. Radiative cooling/heating is parameterized with metallicity, which the ratio of heavier elements than helium. The larger and metallicity corresponds to the later and early stage of galaxies and universe, respectively.\nIt also affects the time scale of cooling/heating and star formation rate. For instance, star formation happens at dense and cold region. With the strong cooling/heating rate, dense regions are quickly cooled down and generates new stars. Inversely, in the case of a weak cooling/heating, when gas is compressed, it is heated up and prevent new stars from being generated.\nIn the case of cold gas with strong cooling/heating, it can easily make dense regions, which require small timesteps and a lot of integration steps. That makes it difficult to get the resolution higher.\nHow to evaluate a new simulator operating in this space:\nThe new simulator should be able to detect potential regions of star formation / potential number of newborn stars, because star formation regions are very dense and need very small timesteps, which results in a massive number of calculation steps.", "id": "Turbulent Interstellar medium in galaxies", "primary_category": "physics.flu-dyn", "authors": "['Keiya Hirashima', 'Kana Moriwaki', 'Michiko S Fujii', 'Yutaka Hirai', 'Takayuki R Saitoh', 'Junichiro Makino']"}
{"dataset": "turbulent_radiative_layer_2D", "url": "https://iopscience.iop.org/article/10.3847/2041-8213/ab8d2c/pdf", "links": "https://iopscience.iop.org/article/10.3847/2041-8213/ab8d2c/pdf", "abstract": "In this simulation, there is cold, dense gas on the bottom and hot dilute gas on the top. They are moving relative to each other at highly subsonic velocities. This set up is unstable to the Kelvin Helmholtz instability, which is seeded with small scale noise that is varied between the simulations. The hot gas and cold gas are both in thermal equilibrium in the sense that the heating and cooling are exactly balanced. However, once mixing occurs as a result of the turbulence induced by the Kelvin Helmholtz instability the intermediate temperatures become populated. This intermediate temperature gas is not in thermal equilibrium and cooling beats heating. This leads to a net mass flux from the hot phase to the cold phase. This process occurs in the interstellar medium, and in the Circum-Galactic medium when cold clouds move through the ambient, hot medium. By understanding how the total cooling and mass transfer scale with the cooling rate we are able to constrain how this process controls the overall phase structure, energetics and dynamics of the gas in and around galaxies.\nDimension of discretized data: 101 timesteps of 384x128 images.\nFields available in the data: Density (scalar field), pressure (scalar field), velocity (vector field).\nNumber of trajectories: 90 (10 different seeds for each of the 9 $t_{cool}$ values).\nEstimated size of the ensemble of all simulations: 6.9 GB.\nGrid type: uniform, cartesian coordinates.\nInitial conditions: Analytic, described in the [paper](https://ui.adsabs.harvard.edu/abs/2020ApJ...894L..24F/abstract).\nBoundary conditions: Periodic in the x-direction, zero-gradient for the y-direction.\nSimulation time-step ($\\Delta t$): varies with $t_{cool}$. Smallest $t_{cool}$ has $\\Delta t = 1.36\\times10^{-2}$ and largest $t_{cool}$ has $\\Delta t = 1.74\\times10^{-2}$. Not that this is not in seconds. This is in dimensionless simulation time.\nData are stored separated by ($\\delta t$): 1.597033 in simulation time.\nTotal time range ($t_{min}$ to $t_{max}$): $t_{min} = 0$, $t_{max} = 159.7033$.\nSpatial domain size ($L_x$, $L_y$, $L_z$): $x \\in [-0.5, 0.5]$, $y \\in [-1, 2]$ giving $L_x = 1$ and $L_y = 3$.\nSet of coefficients or non-dimensional parameters evaluated: $t_{cool} = \\{0.03, 0.06, 0.1, 0.18, 0.32, 0.56, 1.00, 1.78, 3.16\\}$. \nApproximate time to generate the data: 84 seconds using 48 cores for one simulation. 100 CPU hours for everything.\nHardware used to generate the data: 48 CPU cores.\nWhat phenomena of physical interest are catpured in the data:\n-\tThe mass flux from hot to cold phase.\n-\tThe turbulent velocities.\n-\tAmount of mass per temperature bin (T = press/dens).\nHow to evaluate a new simulator operating in this space: See whether it captures the right mass flux, the right turbulent velocities, and the right amount of mass per temperature bin.", "id": "Turbulent Radiative Layer - 2D", "primary_category": "physics.flu-dyn", "authors": "['Drummond B. Fielding', 'Eve C. Ostriker', 'Greg L. Bryan', 'Adam S. Jermyn']"}
{"dataset": "turbulent_radiative_layer_3D", "url": "https://iopscience.iop.org/article/10.3847/2041-8213/ab8d2c/pdf", "links": "https://iopscience.iop.org/article/10.3847/2041-8213/ab8d2c/pdf", "abstract": "In this simulation, there is cold, dense gas on the bottom and hot dilute gas on the top. They are moving relative to each other at highly subsonic velocities. This set up is unstable to the Kelvin Helmholtz instability, which is seeded with small scale noise that is varied between the simulations. The hot gas and cold gas are both in thermal equilibrium in the sense that the heating and cooling are exactly balanced. However, once mixing occurs as a result of the turbulence induced by the Kelvin Helmholtz instability, the intermediate temperatures become populated. This intermediate temperature gas is not in thermal equilibrium, and cooling beats heating. This leads to a net mass flux from the hot phase to the cold phase. This process occurs in the interstellar medium, and in the Circum-Galactic medium when cold clouds move through the ambient, hot medium. By understanding how the total cooling and mass transfer scale with the cooling rate, we are able to constrain how this process controls the overall phase structure, energetics and dynamics of the gas in and around galaxies.\nDimension of discretized data: 101 timesteps of 256x128x128 arrays.\nFields available in the data: Density (scalar field), pressure (scalar field), velocity (vector field).\nNumber of trajectories: 90 trajectories (10 different seeds for each of the 9 $t_{cool}$ variations).\nEstimated size of the ensemble of all simulations: 744.6 GB.\nGrid type: uniform, cartesian coordinates.\nInitial conditions: Analytic, described in the [paper](https://ui.adsabs.harvard.edu/abs/2020ApJ...894L..24F/abstract).\nBoundary conditions: periodic for the 128x128 directions ($x,y$), and zero-gradient for the 256 direction ($z$).\nSimulation time-step: varies with $t_{cool}$. Smallest $t_{cool}$ is $1.32.10^{-2}$, largest $t_{cool}$ is $1.74.10^{-2}$. This is not in seconds, as this is a dimensionless simulation time. To convert, the code time is $L_{box}/cs_{hot}$, where $L_{box}$= 1 parsec and cs_{hot}=100km/s.\nData are stored separated by ($\\Delta t$): data is separated by intervals of simulation time of 2.661722.\nTotal time range ($t_{min}$ to $t_{max}$): $t_{min} = 0$, $t_{max} = 266.172178$.\nSpatial domain size ($L_x$, $L_y$, $L_z$): $x,y\\in\\[-0.5,0.5\\]$, $z\\in\\[-1,1\\]$.\nSet of coefficients or non-dimensional parameters evaluated: $t_{cool} = \\{0.03, 0.06, 0.1, 0.18, 0.32, 0.56, 1.00, 1.78, 3.16\\}$.\nApproximate time to generate the data: $34,560$ CPU hours for all simulations.\nHardware used to generate the data: each simulation was generated on a 128 core \"Rome\" node.\nWhat phenomena of physical interest are catpured in the data: Capte the mass flux from hot to cold phase. Capture turbulent velocities. Capture the amount of mass per temperature bin ($T = \frac{P}{\rho}$).\nHow to evaluate a new simulator operating in this space: Check whether the above physical phenomena are captured by the algorithm.", "id": "Turbulent Radiative Mixing Layers - 3D", "primary_category": "physics.flu-dyn", "authors": "['Drummond B. Fielding', 'Eve C. Ostriker' , 'Greg L. Bryan', 'Adam S. Jermyn']"}
{"dataset": "viscoelastic_instability", "url": "https://www.cambridge.org/core/services/aop-cambridge-core/content/view/D63B7EDB638451A6FC2FBBFDA85E1BBD/S0022112024000508a.pdf/multistability-of-elasto-inertial-two-dimensional-channel-flow.pdf", "links": "https://www.cambridge.org/core/services/aop-cambridge-core/content/view/D63B7EDB638451A6FC2FBBFDA85E1BBD/S0022112024000508a.pdf/multistability-of-elasto-inertial-two-dimensional-channel-flow.pdf", "abstract": "Elasto-inertial turbulence (EIT) is a recently discovered two-dimensional chaotic flow state observed in dilute polymer solutions. Two-dimensional direct numerical simulations show (up to) four coexistent attractors: the laminar state (LAM), a steady arrowhead regime (SAR), Elasto-inertial turbulence (EIT) and a ‘chaotic arrowhead regime’ (CAR). The SAR is stable for all parameters considered here, while the final pair of (chaotic) flow states are visually very similar and can be distinguished only by the presence of a weak polymer arrowhead structure in the CAR regime. Both chaotic regimes are maintained by an identical near-wall mechanism and the weak arrowhead does not play a role. The data set includes snapshots on the four attractors as well as two edge states. An edge state is an unstable state that exists on the boundary between two basins of attractors, the so-called edge manifold. Edge states have a single unstable direction out of the manifold and are relevant since the lie exactly on the boundary separating qualitatively different behaviours of the flow. The edge states in the present data set are obtained through edge tracking between the laminar state and EIT and between EIT and SAR.\nDimension of discretized data: \n- EIT: 34 trajectories with 60 timesteps, 512x512 images (chaotic solution). \n- CAR: 39 trajectories with 60 timesteps, 512x512 images (chaotic solution).\n- SAR: 20 trajectories with 20 timesteps, 512x512 images (simple periodic solutions). \n- Transition to chaos between EIT and SAR: 36 snapshots with 20 timesteps of 512x512 images. \n- Transition to non-chaotic state between EIT and SAR: 38 snapshots with 20 timesteps of 512x512 images. \n- Transition to chaos between EIT and Laminar: 43 snapshots with 20 timesteps of 512x512 images. \n- Transition to non-chaotic state between EIT and Laminar: 49 snapshots with 20 timesteps of 512x512 images.\nFields available in the data: pressure (scalar field), velocity (vector field), positive conformation tensor ( $c\\_{xx}^{\\*}, c^{\\*}\\_{yy},, c^{\\*}\\_{xy}$ are in tensor fields, $c^{\\*}\\_{zz}$ in scalar fields).\nNumber of trajectories: 260 trajectories.\nEstimated size of the ensemble of all simulations: 66 GB.\nGrid type: uniform cartesian coordinates.\nInitial conditions:\n- Edge trajectory: linear interpolation between a chaotic and a non-chaotic state. \n- SAR: continuation of the solution obtained through a linear instability at a different parameter set using time-stepping. \n- EIT: laminar state + blowing and suction at the walls. \n- CAR: SAR + blowing and suction at the walls.\nBoundary conditions: no slip conditions for the velocity ( $(u^\\*,v^\\*)=(0,0)$ ) at the wall and $\\epsilon=0$ at the wall for the equation for $\\mathbf{C^*}$.\nSimulation time-step: various in the different states, but typically $\\sim 10^{-4}$.\nData are stored separated by ($\\Delta t$): various at different states, but typically 1.\nTotal time range ($t_{min}$ to $t_{max}$): depends on the simulation.\nSpatial domain size ($L_x$, $L_y$, $L_z$): $0 \\leq x \\leq 2\\pi$, $-1 \\leq y \\leq 1$.\nSet of coefficients or non-dimensional parameters evaluated: Reynold number $Re=1000$, Weissenberg number $Wi = 50$, $\beta =0.9$, $\\epsilon=2.10^{-6}$, $L_{max}=70$.\nApproximate time to generate the data: 3 months to generate all the data. It takes typically 1 day to generate $\\sim 50$ snapshots.\nHardware used to generate the data: typically 32 or 64 cores.\nWhat phenomena of physical interest are catpured in the data: The phenomena of interest in the data is: (i) chaotic dynamics in viscoelastic flows in EIT and CAR. Also note that they are separate states. (ii) multistability for the same set of parameters, the flow has four different behaviours depending on the initial conditions.\nHow to evaluate a new simulator operating in this space:\nA new simulator would need to capture EIT/CAR adequately for a physically relevant parameter range.", "id": "Multistability of viscoelastic fluids in a 2D channel flow", "primary_category": "physics.flu-dyn", "authors": "['Miguel Beneitez', 'Jacob Page', 'Yves Dubief', 'Rich R. Kerswel']"}
